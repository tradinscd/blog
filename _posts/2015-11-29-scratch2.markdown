---
layout: post
title: Towards Principled Unsupervised Learning
date: '2015-11-29 08:30:46'
---

* Ilya Sutskever, Rafal Jozefowicz, Karol Gregor, Danilo Rezende, Tim Lillicrap, and Oriol Vinyals. Towards Principled Unsupervised Learning. [arXiv preprint arXiv:1511.06440](http://arxiv.org/abs/1511.06440), 2015.

## Summary

The authors argue unsupervised learning is a difficult task bec

## Discussion

What they're really addressing is not so much unsupervised learning but semi-supervised learning. But in Bayesian statistics, the semi-supervised objective is simple:
$$
p(y\_l\mid x\_l, x)
\propto
\int p(y\_l, y, x\_l, x) \operatorname{d}y
$$

However, to address the longstanding motivation, I don't believe there needs to be a one "true" cost function for everything. For instance, this is not even solved in supervised learning: do we minimize mean squared error, median error, or some weird quantile, over the training data? Moreover, how do we acknowledge that minimizing training error is only a proxy for minimizing the true cost function which is the _generalization error_? There are multiple ways to consider regularization (just as there are multiple ways to pose priors), and it's not always clear how to rigorously specify these terms. In both Bayesian and frequentist statistics, we choose the prior and penalty respectively such that these are "appropriate" to the domain problem at hand.

Thus why can we not unify unsupervised learning under the more simple idea of aiming to maximize the likelihood having generated the data $x$, given some probability distribution $p(x)$?